import asyncio
import json
import os
import shutil
import socket
import ssl
import subprocess
import tempfile
import textwrap
from datetime import datetime

from django.conf import settings
from django.core.management import call_command
from django.core.management.base import BaseCommand, CommandError
from django.utils.timezone import make_aware

from findings.models import Endpoint, Finding
from project.models import Asset, Project
from project.scan_utils import resolve_uuids, add_common_scan_arguments
from . import utils




class Command(BaseCommand):
    help = 'Run pentest web scan: ensure ports exist (nmap if needed), optionally crawl with Katana, store Endpoints.'

    def add_arguments(self, parser):
        parser.add_argument('--projectid', type=int, help='ID of the project to scan')
        add_common_scan_arguments(parser)
        parser.add_argument('--scope', type=str, help='Filter by scope (e.g., external, internal)', required=False)
        parser.add_argument('--new-assets', action='store_true', help='Only scan assets with empty last_scan_time')
        parser.add_argument(
            '--katana',
            action='store_true',
            default=True,
            help='Run Katana crawler on root URLs (default: True)',
        )
        parser.add_argument(
            '--no-katana',
            action='store_false',
            dest='katana',
            help='Skip Katana; only collect root endpoints',
        )
        parser.add_argument(
            '--ai',
            action='store_true',
            help='Enable AI analysis of Katana responses to find leaks (passwords, API keys, tokens, etc.)',
        )

    def handle(self, *args, **options):
        assets = self._get_assets_to_scan(**options)
        if not assets.exists():
            self.stdout.write('No assets to scan.')
            return

        run_katana = options.get('katana', True)
        use_ai = options.get('ai', False)
        
        if run_katana and not getattr(settings, 'KATANA_PATH', None):
            raise CommandError('KATANA_PATH is not set in settings. Set it or use --no-katana.')
        
        if use_ai:
            utils.init_ai_client()

        for asset in assets:
            self._process_asset(asset, run_katana=run_katana, use_ai=use_ai, options=options)

    def _get_assets_to_scan(self, **kwargs):
        projectid = kwargs.get('projectid')
        scope_filter = kwargs.get('scope')
        new_assets_only = kwargs.get('new_assets')

        if projectid:
            try:
                project = Project.objects.get(id=projectid)
                qs = Asset.objects.filter(monitor=True, related_project=project)
            except Project.DoesNotExist:
                raise CommandError(f"Project with ID {projectid} does not exist.")
        else:
            qs = Asset.objects.filter(monitor=True)

        uuid_list = resolve_uuids(kwargs)
        if uuid_list:
            qs = qs.filter(uuid__in=uuid_list)
        if scope_filter:
            qs = qs.filter(scope=scope_filter)
        if new_assets_only:
            qs = qs.filter(last_scan_time__isnull=True)

        return qs

    def _detect_protocol(self, domain, port, timeout=3):
        """Detect if a port supports HTTPS or HTTP by attempting connections."""
        # Try HTTPS (TLS handshake) - don't verify certificate
        try:
            context = ssl.create_default_context()
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE
            with socket.create_connection((domain, port), timeout=timeout) as sock:
                with context.wrap_socket(sock, server_hostname=domain):
                    return "https"
        except Exception:
            pass

        # Try HTTP (plain text request)
        try:
            with socket.create_connection((domain, port), timeout=timeout) as sock:
                sock.sendall(b"HEAD / HTTP/1.1\r\nHost: %b\r\n\r\n" % domain.encode())
                data = sock.recv(10)
                if data:
                    return "http"
        except Exception:
            pass

        return None

    def _get_web_root_urls(self, asset):
        """
        Detect web protocols (HTTP/HTTPS) on ports and return root URLs.
        Uses socket/SSL to probe each port and determine the protocol.
        """
        ports = asset.port_set.all()
        if not ports.exists():
            return []

        urls = []
        for port in ports:
            banner_lower = (port.banner or '').lower()
            if 'http' in banner_lower or 'ssl' in banner_lower:
                protocol = self._detect_protocol(asset.value, port.port)
                if protocol:
                    urls.append(f"{protocol}://{asset.value}:{port.port}")

        return urls

    def _process_asset(self, asset, run_katana, use_ai, options):
        self.stdout.write(f'Asset: {asset.value} ({asset.uuid})')

        # 1) Ensure we have ports; run nmap if none
        if not asset.port_set.exists():
            self.stdout.write('  No ports found; running nmap.')
            call_command(
                'scan_nmap',
                uuids=asset.uuid,
                projectid=options.get('projectid'),
            )
            asset.refresh_from_db()

        root_urls = self._get_web_root_urls(asset)
        if not root_urls:
            self.stdout.write('  No web ports (http/https/ssl); skipping.')
            return

        self.stdout.write(f'  Root URLs: {len(root_urls)}')

        # 2) Optionally run Katana on root URLs
        discovered_urls = []
        response_dir = None
        if run_katana:
            discovered_urls, response_dir = self._run_katana(root_urls, use_ai)
            self.stdout.write(f'  Katana discovered: {len(discovered_urls)} URLs')

        # 3) Delete existing endpoints for this asset, then create new ones
        deleted_count = Endpoint.objects.filter(asset=asset).delete()[0]
        self.stdout.write(f'  Deleted {deleted_count} existing endpoint(s)')

        # 4) Merge root + discovered and create Endpoint records
        all_urls = list(dict.fromkeys(root_urls + discovered_urls))
        created = 0
        for url in all_urls:
            url = (url or '').strip()
            if not url:
                continue
            Endpoint.objects.create(
                url=url,
                asset=asset,
                technologies='',
            )
            created += 1

        self.stdout.write(f'  Endpoints created: {created}')

        # 5) AI analysis of stored responses if enabled
        if use_ai and response_dir:
            self._analyze_responses_with_ai(asset, response_dir)

    def _run_katana(self, urls, use_ai=False):
        """Run Katana with -list input and -jsonl output; return list of discovered URLs and response directory."""
        katana_path = settings.KATANA_PATH
        if not katana_path:
            self.stdout.write('  KATANA_PATH not set; skipping Katana.')
            return [], None

        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:
            for u in urls:
                f.write(u + '\n')
            list_path = f.name

        response_dir = None
        if use_ai:
            response_dir = tempfile.mkdtemp(prefix='katana_responses_')

        try:
            cmd = [katana_path, '-list', list_path, '-jsonl']
            if use_ai and response_dir:
                cmd.extend(['-srd', response_dir])

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                # timeout=3600,
            )

            discovered = []
            for line in (result.stdout or '').strip().splitlines():
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    req = obj.get('request') or {}
                    url = req.get('endpoint') or req.get('url')
                    if url:
                        discovered.append(url)
                except (json.JSONDecodeError, TypeError):
                    pass

            return discovered, response_dir
        except subprocess.TimeoutExpired:
            self.stdout.write(self.style.WARNING('  Katana timed out.'))
            return [], response_dir
        except FileNotFoundError:
            self.stdout.write(self.style.WARNING(f'  Katana not found at {katana_path}.'))
            return [], response_dir
        except Exception as e:
            self.stdout.write(self.style.WARNING(f'  Katana error: {e}'))
            return [], response_dir
        finally:
            try:
                os.unlink(list_path)
            except OSError:
                pass

    def _analyze_responses_with_ai(self, asset, response_dir):
        """Analyze stored HTTP responses with AI to find leaks."""
        if not os.path.exists(response_dir):
            return

        # Delete existing findings from previous scans
        Finding.objects.filter(asset=asset, source='pentest_web_ai').delete()

        self.stdout.write('  Analyzing responses with AI (using interactive file access tools)...')
        self.stdout.write(f'  Response directory: {response_dir}')

        # Create verbose logger function
        def verbose_log(msg):
            self.stdout.write(f'  {msg}')

        # Run AI analysis with file access
        try:
            findings_data = asyncio.run(utils.analyze_with_file_access(response_dir, asset.value, verbose_logger=verbose_log))
            self._store_findings(asset, findings_data)
        except Exception as e:
            self.stdout.write(self.style.ERROR(f'  AI analysis error: {e}'))
            import traceback
            self.stdout.write(self.style.ERROR(f'  Traceback: {traceback.format_exc()}'))
        finally:
            # Cleanup response directory
            try:
                self.stdout.write(f'  Cleaning up response directory: {response_dir}')
                shutil.rmtree(response_dir)
                self.stdout.write('  Response directory cleaned up')
            except Exception as cleanup_error:
                self.stdout.write(self.style.WARNING(f'  Warning: Could not clean up response directory: {cleanup_error}'))

    def _store_findings(self, asset, findings_data):
        """Store findings from AI analysis."""
        payload = findings_data or {}
        results = payload.get('findings', [])
        if not isinstance(results, list):
            return

        self.stdout.write(f'  AI findings identified: {len(results)}')
        for finding in results:
            name = finding.get('name')
            if not name:
                continue

            severity_raw = finding.get('severity', 'Info') or 'Info'
            severity_normalized = severity_raw.lower()
            finding_type = finding.get('type', 'other') or 'other'
            evidence = finding.get('evidence', '') or ''
            reasoning = finding.get('reasoning', '') or ''
            recommendation = finding.get('recommendation', '') or ''
            reference = finding.get('reference', '') or ''

            # Description must start with the URL
            url_prefix = f"URL: {reference}\n\n" if reference else ""
            
            description_parts = [
                reasoning or '',
                f"Evidence: {evidence}" if evidence else '',
                f"Recommendation: {recommendation}" if recommendation else '',
            ]
            description_text = url_prefix + '\n\n'.join([part for part in description_parts if part])

            Finding.objects.create(
                asset=asset,
                asset_name=asset.value,
                keyword=asset.related_keyword,
                source='pentest_web_ai',
                name=name,
                type=finding_type.lower(),
                severity=severity_normalized,
                description=description_text,
                reference=reference,
                scan_date=make_aware(datetime.now()),
                last_seen=make_aware(datetime.now()),
                reported=False,
            )

            self.stdout.write(f'    - Stored finding: {name} ({severity_raw})')
